# Overall Pipeline

## Uncertainty Estimation (check hallucination via self-consistency):
the origin of LLM hallucinations, is inherently tied to the model’s uncertainty. Therefore, by estimating the uncertainty of the factuat content generated by the model, it becomes feasible to detect hallucinations (https://arxiv.org/pdf/2311.05232.pdf)

Single LLM Multiple Generation:
1: Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language
models; Direct Asking （https://github.com/potsawee/selfcheckgpt)

2 : Do language models know when they’re hallucinating references? Indirect Asking
3: SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning (reasoning) (https://github.com/NingMiao/SelfCheck)
4:  Chatgpt as a factual inconsistency evaluator for text summarization
Multi-Agent Approach:
1: LM vs LM: detecting factual errors via cross examination. 

### Note:
GroundTruth Label is manual annotated or use some other approximation. To simplify the work, Should we aviod open-end questions?

## 


In a nutshell, we first adapt our pipeline to some complex questions (Math Reasoning/Generative Question Answering/Multiple Choices/text summarization);    


We proposed a hallucination detector to give scores and rank the hallucination.
We proposed a method to explain the most hallcinated output and compared it with the least hallucinated output.


(A implies B implies C != C implies B implies A)






