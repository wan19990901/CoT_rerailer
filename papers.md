# KG-LLM-CA-Papers

This markdown collects papers related to **LLM Hallucination**,  **LLM Interpretability**, **LLM Agent**, and **LLM and Counterfactual Reasonings**.

   
## Content
  
- [Papers](#papers)
  - [LLM Hallucination](#llm-and-hal)
    - [Surveys](#surveys)
    - [Methods](#methods)
    - [Resources](#resources)
  - [LLM Interpretbility](#llm-and-it)
    - [Surveys](#surveys)
    - [Methods](#methods)
    - [Resources](#resources)
  - [LLM Agent](#llm-and-ag)
    - [Related Methods](#methods)
  - [LLM and CounterFactualReasoning](#llm-and-cf)
    - [Related Methods](#methods)

## Papers

### LLM Hallucination

#### Survey
- \[[arxiv](https://arxiv.org/html/2402.06647v1)\] A Survey on Large Language Model Hallucination via a Creativity Perspective. `2024.2`

- \[[arxiv](https://arxiv.org/abs/2209.11326)\] Towards Faithful Model Explanation in NLP: A Survey. `2024.1`

- \[[arxiv](https://arxiv.org/abs/2401.01313)\] A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models. `2024.1`

- \[[arxiv](https://arxiv.org/abs/2311.05232)\] A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. `2023.11`



#### Source of Hallucination

- \[[arxiv](https://arxiv.org/abs/2401.11817)\] Hallucination is Inevitable: An Innate Limitation of Large Language Models. `2024.1`

- \[[arxiv](https://arxiv.org/abs/2309.13007)\] Why Does ChatGPT Fall Short in Providing Truthful Answers? `2023.12`

- \[[arxiv](https://arxiv.org/abs/2305.14552)\] Sources of Hallucination by Large Language Models on Inference Tasks `2023.10`

#### Related Methods


- \[[arxiv](https://arxiv.org/abs/2308.07758)\] Forward-Backward Reasoning in Large Language Models for Mathematical Verification. `2024.2`
- \[[arxiv](https://arxiv.org/abs/2311.09277)\] Contrastive Chain-of-Thought Prompting `2023.11`
- \[[arxiv](https://arxiv.org/abs/2306.03872)\] Deductive Verification of Chain-of-Thought Reasoning `2023.10`
- \[[arxiv](https://arxiv.org/abs/2212.09561)\] Large Language Models are Better Reasoners with Self-Verification `2023.10`
- \[[arxiv](https://arxiv.org/abs/2308.00436)\] SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning `2023.10`
- \[[arxiv](https://arxiv.org/abs/2310.03951)\] Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations `2023.10`
- \[[arxiv](https://arxiv.org/abs/2303.08896)\] SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models `2023.10`
- \[[arxiv](https://arxiv.org/abs/2305.11747)\] HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models `2023.10`

- \[[arxiv](https://arxiv.org/abs/2305.15852)\] Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation `2023.10`
- \[[arxiv](https://arxiv.org/abs/2309.11495)\] CHAIN-OF-VERIFICATION REDUCES HALLUCINATION IN LARGE LANGUAGE MODELS `2023.9`
- \[[arxiv](https://arxiv.org/abs/2305.13281)\] LM vs LM: Detecting Factual Errors via Cross Examination `2023.5`
https://dl.acm.org/doi/10.1145/3583780.3614905 
- \[[arxiv](https://arxiv.org/abs/2206.14858)\] Solving Quantitative Reasoning Problems with Language Models `2022.6`


### LLM Agents Reasonings

#### Related Methods

- \[[arxiv](https://arxiv.org/abs/2309.13007)\] ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs. `2023.8`

- \[[arxiv](https://arxiv.org/abs/2308.07201)\] ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate. `2023.8`

- \[[arxiv](https://arxiv.org/abs/2305.14325)\] Improving Factuality and Reasoning in Language Models through Multiagent Debate. `2023.5`

- \[[arxiv](https://arxiv.org/abs/2305.11595)\] Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate. `2023.5`

- \[[arxiv](https://arxiv.org/abs/2305.19118)\] Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate. `2023.5`

