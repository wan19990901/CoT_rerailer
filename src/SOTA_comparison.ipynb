{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "def modify_float(num):\n",
    "    try:\n",
    "        return str(round(float(num),1))\n",
    "    except:\n",
    "        return num\n",
    "def compute_results(df):\n",
    "    correctness = 0\n",
    "    sc_correctness = 0\n",
    "    mad_correctness = 0\n",
    "    rerailer_correctness = 0\n",
    "    ltm_correctness = 0\n",
    "    auto_correctness = 0\n",
    "    self_check_correctness = 0\n",
    "    deve_correctness = 0\n",
    "    for row_index in range(len(df)):\n",
    "        row = df.iloc[row_index]\n",
    "        correct_ans = row['Correct_Answer'].lower()\n",
    "        output_answer = row['Cot_random_answer'].lower()\n",
    "        sc_answer = row['Smv_Answer'].lower()\n",
    "        # judge_answer = row['Output_Answer'].lower()\n",
    "        mad_answer = modify_float(row['MAD_Answer']).lower()\n",
    "        rerailer_ans = row['Corrected COT Answer'].lower()\n",
    "        ltm_answer = modify_float(row['ltm_Answer']).lower()\n",
    "        auto_ans = row['auto_Answer'].lower()\n",
    "        self_check_ans = modify_float(row['self_check_Answer']).lower()\n",
    "        deve_ans = modify_float(row['deve_Answer']).lower()\n",
    "        if correct_ans == output_answer[0]:\n",
    "            correctness += 1\n",
    "        else:\n",
    "            correctness += 0\n",
    "        if correct_ans == sc_answer[0]:\n",
    "            sc_correctness += 1\n",
    "        else:\n",
    "            sc_correctness += 0\n",
    "        if correct_ans == mad_answer[0]:\n",
    "            mad_correctness += 1\n",
    "        else:\n",
    "            mad_correctness += 0\n",
    "        if correct_ans == rerailer_ans[0]:\n",
    "            rerailer_correctness += 1\n",
    "        else:\n",
    "            rerailer_correctness += 0\n",
    "        if correct_ans == ltm_answer[0]:\n",
    "            ltm_correctness += 1\n",
    "        else:\n",
    "            ltm_correctness += 0\n",
    "        if correct_ans == auto_ans[0]:\n",
    "            auto_correctness += 1\n",
    "        else:\n",
    "            auto_correctness += 0\n",
    "        if correct_ans == self_check_ans[0]:\n",
    "            self_check_correctness += 1\n",
    "        if correct_ans == deve_ans[0]:\n",
    "            deve_correctness += 1\n",
    "    \n",
    "    result_dict={\n",
    "        'CoT':round(correctness/len(df),3),\n",
    "        'SC':round(sc_correctness/len(df),3),\n",
    "        'FST_CoT':round(mad_correctness/len(df),3),\n",
    "        'Rerailer':round(rerailer_correctness/len(df),3),\n",
    "        'LTM':round(self_check_correctness/len(df),3),\n",
    "        'COVE':round(deve_correctness/len(df),3)\n",
    "    }\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../result/sota_comparison/sota_comp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "Challenging Math    290\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CoT': 0.421,\n",
       " 'SC': 0.428,\n",
       " 'FST_CoT': 0.462,\n",
       " 'Rerailer': 0.517,\n",
       " 'LTM': 0.507,\n",
       " 'COVE': 0.483}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_results(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoT time and API calls per question: 22 1\n",
      "SC time and API calls per question: 880 40\n",
      "FSCOT time and API calls per question: 132 6\n",
      "LTM time and API calls per question: 704 32\n",
      "Rerailer time and API calls per question: 126.0581348405329 27\n",
      "CoVE time and API calls per question: 7481.253469685902 340.0569758948137\n"
     ]
    }
   ],
   "source": [
    "num_of_samples = 1 # 1 question\n",
    "unit_time = 22*num_of_samples # 22 seconds in average per question when using gpt-4 model\n",
    "unit_cost = 0.06*num_of_samples # 0.06 USD in average per question when using gpt-4 model\n",
    "# SC\n",
    "SC_call = 40\n",
    "\n",
    "# MAD\n",
    "num_of_agents = 2\n",
    "rounds = 3\n",
    "MAD_call = num_of_agents*rounds\n",
    "\n",
    "# Rerailer\n",
    "sc_cot_call = 3 # 3 times of SC\n",
    "judge_call = 1 # 1 time of Judge\n",
    "root_checker_call = 10 # Average of 10 steps per question -> 10 times of root checker\n",
    "debate_call = 3 # 3 times of debate\n",
    "check_times = 3 # In average, 3 incorrect steps per question\n",
    "final_cot_call = 1 # 1 time of regernation\n",
    "consistent_inconsistent_ratio = 322/2477 # Based on the final result, 322 inconsistent out of 2477 questions\n",
    "\n",
    "derailment_call = sc_cot_call\n",
    "rerailment_call = (judge_call+root_checker_call+(debate_call)*check_times + final_cot_call)\n",
    "Rerailer_call = derailment_call + rerailment_call*consistent_inconsistent_ratio\n",
    "min_call = derailment_call # Derailment only\n",
    "max_call = (judge_call+root_checker_call+(debate_call+1)*check_times + final_cot_call) + derailment_call # Worst case scenario\n",
    "\n",
    "\n",
    "# Self-Check\n",
    "sc_cot = 1\n",
    "sc_stages = 3\n",
    "sc_regenerate = 1\n",
    "self_check_call = root_checker_call*sc_stages + sc_cot + sc_regenerate\n",
    "\n",
    "# Deductive verification\n",
    "num_of_ver = 7.504504504504505 # determined from the data distribution provided by author\n",
    "num_of_check_rounds = 4.738738738738738 # determined from the data distribution provided by author\n",
    "np = 10\n",
    "num_of_check_criteria = 3\n",
    "num_of_check_criteria_per_round = 3\n",
    "deductive_call = np + np + num_of_ver*num_of_check_rounds*num_of_check_criteria*num_of_check_criteria_per_round\n",
    "\n",
    "\n",
    "\n",
    "print('CoT time and API calls per question:',1*unit_time,1)\n",
    "print('SC time and API calls per question:', SC_call*unit_time,SC_call)\n",
    "print('FSCOT time and API calls per question:',MAD_call*unit_time,MAD_call)\n",
    "print('LTM time and API calls per question:',self_check_call*unit_time,self_check_call)\n",
    "print('Rerailer time and API calls per question:',Rerailer_call*unit_time,max_call)\n",
    "print('CoVE time and API calls per question:',deductive_call*unit_time,deductive_call)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_hall_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
