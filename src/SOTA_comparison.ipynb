{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "def modify_float(num):\n",
    "    try:\n",
    "        return str(round(float(num),1))\n",
    "    except:\n",
    "        return num\n",
    "def compute_results(df):\n",
    "    correctness = 0\n",
    "    sc_correctness = 0\n",
    "    mad_correctness = 0\n",
    "    rerailer_correctness = 0\n",
    "    ltm_correctness = 0\n",
    "    auto_correctness = 0\n",
    "    self_check_correctness = 0\n",
    "    deve_correctness = 0\n",
    "    for row_index in range(len(df)):\n",
    "        row = df.iloc[row_index]\n",
    "        correct_ans = row['Correct_Answer'].lower()\n",
    "        output_answer = row['Cot_random_answer'].lower()\n",
    "        sc_answer = row['Smv_Answer'].lower()\n",
    "        # judge_answer = row['Output_Answer'].lower()\n",
    "        mad_answer = modify_float(row['MAD_Answer']).lower()\n",
    "        rerailer_ans = row['Corrected COT Answer'].lower()\n",
    "        ltm_answer = modify_float(row['ltm_Answer']).lower()\n",
    "        auto_ans = row['auto_Answer'].lower()\n",
    "        self_check_ans = modify_float(row['self_check_Answer']).lower()\n",
    "        deve_ans = modify_float(row['deve_Answer']).lower()\n",
    "        if correct_ans == output_answer[0]:\n",
    "            correctness += 1\n",
    "        else:\n",
    "            correctness += 0\n",
    "        if correct_ans == sc_answer[0]:\n",
    "            sc_correctness += 1\n",
    "        else:\n",
    "            sc_correctness += 0\n",
    "        if correct_ans == mad_answer[0]:\n",
    "            mad_correctness += 1\n",
    "        else:\n",
    "            mad_correctness += 0\n",
    "        if correct_ans == rerailer_ans[0]:\n",
    "            rerailer_correctness += 1\n",
    "        else:\n",
    "            rerailer_correctness += 0\n",
    "        if correct_ans == ltm_answer[0]:\n",
    "            ltm_correctness += 1\n",
    "        else:\n",
    "            ltm_correctness += 0\n",
    "        if correct_ans == auto_ans[0]:\n",
    "            auto_correctness += 1\n",
    "        else:\n",
    "            auto_correctness += 0\n",
    "        if correct_ans == self_check_ans[0]:\n",
    "            self_check_correctness += 1\n",
    "        if correct_ans == deve_ans[0]:\n",
    "            deve_correctness += 1\n",
    "    \n",
    "    result_dict={\n",
    "        'CoT':round(correctness/len(df),3),\n",
    "        'SC':round(sc_correctness/len(df),3),\n",
    "        'MAD':round(mad_correctness/len(df),3),\n",
    "        'Rerailer':round(rerailer_correctness/len(df),3),\n",
    "        # 'LTM':round(ltm_correctness/len(df),3),\n",
    "        # 'Auto':round(rerailer_correctness/len(df),3),\n",
    "        'Self_Check':round(self_check_correctness/len(df),3),\n",
    "        'Deve':round(deve_correctness/len(df),3)\n",
    "    }\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../result/sota_comparison/sota_comp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "Challenging Math    290\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CoT': 0.421,\n",
       " 'SC': 0.428,\n",
       " 'MAD': 0.462,\n",
       " 'Rerailer': 0.517,\n",
       " 'Self_Check': 0.507,\n",
       " 'Deve': 0.483}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_results(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoT 60.0 6.111111111111111 1\n",
      "SC 2400.0 244.44444444444446 40\n",
      "MAD 360.0 36.666666666666664 6\n",
      "Rerailer 343.79491320145337 35.01614856681469 5.729915220024223\n",
      "Self Check 1920.0 195.55555555555554 32\n",
      "Deductive Verification 20403.41855368882 2078.1259638016395 340.0569758948137\n"
     ]
    }
   ],
   "source": [
    "num_of_samples = 1000\n",
    "unit_time = 22*num_of_samples\n",
    "unit_cost = 0.06*num_of_samples\n",
    "# SC\n",
    "SC_call = 40\n",
    "\n",
    "# MAD\n",
    "num_of_agents = 2\n",
    "rounds = 3\n",
    "MAD_call = num_of_agents*rounds\n",
    "# Rerailer\n",
    "cot_call = 3\n",
    "judge_call = 1\n",
    "root_checker_call = 10\n",
    "debate_call = 2\n",
    "check_times = 3\n",
    "final_cot_call = 1\n",
    "consistent_inconsistent_ratio = 322/2477\n",
    "\n",
    "derailment_call = cot_call\n",
    "rerailment_call = (judge_call+root_checker_call+(debate_call+1)*check_times + final_cot_call)*consistent_inconsistent_ratio\n",
    "Rerailer_call = derailment_call + rerailment_call\n",
    "min_call = derailment_call\n",
    "max_call = (judge_call+root_checker_call+(debate_call+1)*check_times + final_cot_call)\n",
    "\n",
    "\n",
    "# Self-Check\n",
    "sc_cot = 1\n",
    "sc_stages = 3\n",
    "sc_regenerate = 1\n",
    "self_check_call = root_checker_call*sc_stages + sc_cot + sc_regenerate\n",
    "\n",
    "# Deductive verification\n",
    "# import json\n",
    "# with open('../data/gsm8k_verification.json','r') as f:\n",
    "#     data = json.load(f)\n",
    "# num_of_ver_li = []\n",
    "# num_of_check_rounds_li = []\n",
    "# for d in range(len(data)):\n",
    "#     num_of_ver_li.append(len(data[d]['verify_model_results']))\n",
    "#     num_of_check_rounds_li.append(len(data[d]['verify_model_results'][0]))\n",
    "# num_of_ver = sum(num_of_ver_li)/len(num_of_ver_li)\n",
    "# num_of_check_rounds = sum(num_of_check_rounds_li)/len(num_of_check_rounds_li)\n",
    "\n",
    "num_of_ver = 7.504504504504505 # determined from the data distribution provided by author\n",
    "num_of_check_rounds = 4.738738738738738 # determined from the data distribution provided by author\n",
    "np = 10\n",
    "num_of_check_criteria = 3\n",
    "num_of_check_criteria_per_round = 3\n",
    "deductive_call = np + np + num_of_ver*num_of_check_rounds*num_of_check_criteria*num_of_check_criteria_per_round\n",
    "\n",
    "\n",
    "\n",
    "print('CoT',1*unit_cost,1*unit_time/3600,1)\n",
    "print('SC',SC_call*unit_cost,SC_call*unit_time/3600,SC_call)\n",
    "print('MAD',MAD_call*unit_cost,MAD_call*unit_time/3600,MAD_call)\n",
    "print('Rerailer',Rerailer_call*unit_cost,Rerailer_call*unit_time/3600,Rerailer_call)\n",
    "print('Self Check',self_check_call*unit_cost,self_check_call*unit_time/3600,self_check_call)\n",
    "print('Deductive Verification',deductive_call*unit_cost,deductive_call*unit_time/3600,deductive_call)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_hall",
   "language": "python",
   "name": "llm_hall_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
